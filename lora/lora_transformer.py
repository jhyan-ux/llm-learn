"""
LoRAç‰ˆæœ¬çš„Transformerç»„ä»¶
é›†æˆLoRAåˆ°MultiHeadAttentionä¸­ï¼Œåªå¯¹W_qå’ŒW_våº”ç”¨LoRA
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from lora_layer import LoRALinear
from transformer.transformer import MultiHeadAttention, EncoderLayer, TransformerEncoder
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class LoRAMultiHeadAttention(MultiHeadAttention):
    """
    é›†æˆLoRAçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    åªå¯¹W_qå’ŒW_våº”ç”¨LoRAï¼ŒW_kå’ŒW_oä¿æŒåŸå§‹Linearå±‚
    """
    
    def __init__(self, d_model, num_heads, lora_rank=16):
        """
        åˆå§‹åŒ–LoRAç‰ˆæœ¬çš„å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°  
            lora_rank: LoRAçš„ç§©å‚æ•°
        """
        # TODO 1: å¦‚ä½•æ­£ç¡®åˆå§‹åŒ–çˆ¶ç±»ï¼Ÿ
        # ä½ éœ€è¦è°ƒç”¨çˆ¶ç±»çš„__init__ï¼Œè¿™æ ·ä¼šåˆ›å»ºåŸå§‹çš„W_q, W_k, W_v, W_o
        super().__init__(d_model, num_heads)
        
        # TODO 2: æ›¿æ¢ç‰¹å®šçš„çº¿æ€§å±‚ä¸ºLoRAç‰ˆæœ¬
        # ä¿å­˜lora_rankä»¥å¤‡åç”¨
        self.lora_rank = lora_rank
        
        # æ›¿æ¢W_qä¸ºLoRAç‰ˆæœ¬
        # æ€è€ƒï¼šåŸå§‹çš„W_qå·²ç»åœ¨çˆ¶ç±»ä¸­åˆ›å»ºäº†ï¼Œç°åœ¨éœ€è¦æ›¿æ¢å®ƒ
        # ä½ éœ€è¦ç”¨LoRALinearæ›¿æ¢self.W_q
        original_W_q = self.W_q  # ä¿å­˜åŸæ¥çš„æƒé‡ä»¥ä¾¿å¤åˆ¶
        self.W_q = LoRALinear(d_model, d_model, lora_rank)
        
        # TODO 3: å¦‚ä½•å°†é¢„è®­ç»ƒæƒé‡å¤åˆ¶åˆ°LoRAå±‚ï¼Ÿ
        # æç¤ºï¼šLoRALinearç»§æ‰¿äº†nn.Linearï¼Œæ‰€ä»¥æœ‰weightå’Œbiaså±æ€§
        # éœ€è¦å°†original_W_qçš„æƒé‡å¤åˆ¶åˆ°æ–°çš„LoRAå±‚ä¸­
        with torch.no_grad():
            self.W_q.weight.copy_(original_W_q.weight)
            if original_W_q.bias is not None:
                self.W_q.bias.copy_(original_W_q.bias)
        
        # TODO 4: å¯¹W_våšåŒæ ·çš„æ›¿æ¢
        original_W_v = self.W_v
        self.W_v = LoRALinear(d_model, d_model, lora_rank)
        
        with torch.no_grad():
            self.W_v.weight.copy_(original_W_v.weight)
            if original_W_v.bias is not None:
                self.W_v.bias.copy_(original_W_v.bias)
        
        self.W_k.weight.requires_grad = False
        self.W_k.bias.requires_grad = False
        self.W_o.weight.requires_grad = False
        self.W_o.bias.requires_grad = False
        # æ³¨æ„ï¼šW_kå’ŒW_oä¿æŒåŸæ ·ï¼Œä¸åº”ç”¨LoRA
        
    def forward(self, query, key, value, mask=None):
        return super().forward(query, key, value, mask)
        
    def get_lora_parameters(self):
        """
        è·å–æ‰€æœ‰LoRAå‚æ•°ï¼Œç”¨äºå•ç‹¬ä¿å­˜æˆ–ä¼˜åŒ–
        """
        lora_params = []
        # TODO 7: æ”¶é›†æ‰€æœ‰LoRAç›¸å…³çš„å‚æ•°
        # æç¤ºï¼šéœ€è¦æ”¶é›†W_qå’ŒW_vçš„lora_Aå’Œlora_Bå‚æ•°
        lora_params.extend([self.W_q.lora_A, self.W_q.lora_B])  # W_qçš„LoRAå‚æ•°
        lora_params.extend([self.W_v.lora_A, self.W_v.lora_B])  # W_vçš„LoRAå‚æ•°
        return lora_params
        
    def merge_and_save(self, path):
        """
        å°†LoRAæƒé‡åˆå¹¶åˆ°åŸå§‹æƒé‡ä¸­å¹¶ä¿å­˜
        ç”¨äºæ¨ç†æ—¶çš„ä¼˜åŒ–
        """
        # TODO 8: å®ç°æƒé‡åˆå¹¶é€»è¾‘
        # æç¤ºï¼šå¯¹äºæ¯ä¸ªLoRAå±‚ï¼Œè®¡ç®—W_new = W_original + scaling * B @ A
        W_q_new = self.W_q.weight + self.W_q.lora_B @ self.W_q.lora_A * self.W_q.scaling 
        W_v_new = self.W_v.weight + self.W_v.lora_B @ self.W_v.lora_A * self.W_v.scaling 

        self.W_q.weight.copy_(W_q_new)
        self.W_v.weight.copy_(W_v_new)

        torch.save(self.state_dict(), path)

        

# æµ‹è¯•ä»£ç 
def test_lora_attention():
    """æµ‹è¯•LoRAæ³¨æ„åŠ›å±‚"""
    print("æµ‹è¯•LoRAå¤šå¤´æ³¨æ„åŠ›å±‚...")
    
    try:
        # å‚æ•°è®¾ç½®
        d_model = 512
        num_heads = 8
        lora_rank = 16
        batch_size = 2
        seq_len = 10
        
        print(f"å‚æ•°: d_model={d_model}, num_heads={num_heads}, lora_rank={lora_rank}")
        
        # 1. åˆ›å»ºLoRAæ³¨æ„åŠ›å±‚
        lora_attention = LoRAMultiHeadAttention(d_model, num_heads, lora_rank)
        print("âœ“ LoRAæ³¨æ„åŠ›å±‚åˆ›å»ºæˆåŠŸ")
        
        # 2. åˆ›å»ºæµ‹è¯•è¾“å…¥ (æ¨¡æ‹Ÿtoken embeddings)
        query = torch.randn(batch_size, seq_len, d_model)
        key = torch.randn(batch_size, seq_len, d_model)  
        value = torch.randn(batch_size, seq_len, d_model)
        print(f"âœ“ æµ‹è¯•è¾“å…¥åˆ›å»ºæˆåŠŸ: {query.shape}")
        
        # 3. å‰å‘ä¼ æ’­
        output = lora_attention(query, key, value)
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # 4. æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, seq_len, d_model)
        assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} vs {expected_shape}"
        print("âœ“ è¾“å‡ºå½¢çŠ¶éªŒè¯é€šè¿‡")
        
        # 5. éªŒè¯LoRAå‚æ•°æ˜¯å¦å¯è®­ç»ƒ
        lora_params = lora_attention.get_lora_parameters()
        print(f"âœ“ LoRAå‚æ•°æ•°é‡: {len(lora_params)}")
        
        for i, param in enumerate(lora_params):
            print(f"  å‚æ•°{i+1}: {param.shape}, requires_grad={param.requires_grad}")
            assert param.requires_grad, f"LoRAå‚æ•°{i+1}ä¸å¯è®­ç»ƒï¼"
        
        # 6. éªŒè¯åŸå§‹æƒé‡è¢«å†»ç»“
        assert not lora_attention.W_k.weight.requires_grad, "W_kåº”è¯¥è¢«å†»ç»“"
        assert not lora_attention.W_o.weight.requires_grad, "W_oåº”è¯¥è¢«å†»ç»“"  
        assert not lora_attention.W_q.weight.requires_grad, "W_qçš„åŸå§‹æƒé‡åº”è¯¥è¢«å†»ç»“"
        assert not lora_attention.W_v.weight.requires_grad, "W_vçš„åŸå§‹æƒé‡åº”è¯¥è¢«å†»ç»“"
        print("âœ“ æƒé‡å†»ç»“éªŒè¯é€šè¿‡")
        
        # 7. è®¡ç®—å‚æ•°æ•ˆç‡
        original_params = 4 * d_model * d_model  # 4ä¸ªçº¿æ€§å±‚
        lora_params_count = 2 * lora_rank * (d_model + d_model)  # åªæœ‰W_qå’ŒW_vç”¨LoRA
        efficiency = (original_params - lora_params_count) / original_params * 100
        
        print(f"âœ“ å‚æ•°æ•ˆç‡åˆ†æ:")
        print(f"  åŸå§‹å‚æ•°: {original_params:,}")
        print(f"  LoRAå‚æ•°: {lora_params_count:,}") 
        print(f"  å‚æ•°å‡å°‘: {efficiency:.1f}%")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LoRAæ³¨æ„åŠ›å±‚å·¥ä½œæ­£å¸¸ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_lora_attention() 
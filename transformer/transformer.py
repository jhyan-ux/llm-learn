import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        # TODO: ä½ éœ€è¦åœ¨è¿™é‡Œå®šä¹‰ä»€ä¹ˆå‚æ•°ï¼Ÿ
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = self.d_model // self.num_heads # å¤šå¤´ä¸­æ¯ä¸€ä¸ªå¤´çš„ç»´åº¦
        
        # æ³¨æ„ï¼šè¿™äº›çº¿æ€§å±‚è¦ä¸ºæ‰€æœ‰å¤´åŒæ—¶ç”ŸæˆQã€Kã€V
        self.W_q = nn.Linear(d_model, d_model)  # è¾“å‡ºæ‰€æœ‰å¤´çš„Query
        self.W_k = nn.Linear(d_model, d_model)  # è¾“å‡ºæ‰€æœ‰å¤´çš„Key
        self.W_v = nn.Linear(d_model, d_model)  # è¾“å‡ºæ‰€æœ‰å¤´çš„Value
        self.W_o = nn.Linear(d_model, d_model)  # åˆå¹¶æ‰€æœ‰å¤´çš„è¾“å‡º

    
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len = query.size(0), query.size(1)
        
        # 1. ç”ŸæˆQã€Kã€VçŸ©é˜µ
        Q = self.W_q(query)  # (batch_size, seq_len, d_model)
        K = self.W_k(key)    # (batch_size, seq_len, d_model)
        V = self.W_v(value)  # (batch_size, seq_len, d_model)
        
        # 2. Reshapeä¸ºå¤šå¤´æ ¼å¼å¹¶è½¬ç½®
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)
        
        # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        score = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)
        if mask is not None:
            score = score.masked_fill(mask == 0, -1e9)
        attention_weights = F.softmax(score, dim=-1)
        attention_output = attention_weights @ V # (batch_size, num_heads, seq_len, d_k)
        
        # 4. åˆå¹¶å¤šå¤´ç»“æœ
        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_k * self.num_heads)
        output = self.W_o(attention_output)
        
        return output

class PositionEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length=5000):
        super().__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) # (max_seq_length, 1)
        i = torch.arange(0, d_model, 2, dtype=torch.float) # (d_model // 2)
        
        # ä¿®æ­£å…¬å¼ï¼šæ³¨æ„æ‹¬å·å’Œé¡ºåº
        div_term = torch.exp(i * (-math.log(10000.0) / d_model))
        
        # å¶æ•°ä½ç½®ç”¨sinï¼Œå¥‡æ•°ä½ç½®ç”¨cos
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½® [0,2,4,6,...]
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½® [1,3,5,7,...]
        
        # æ³¨å†Œä¸ºbufferï¼Œè·Ÿéšæ¨¡å‹ç§»åŠ¨
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        # pe: (max_seq_len, d_model)
        # å–å‡ºéœ€è¦çš„é•¿åº¦å¹¶æ·»åŠ åˆ°è¾“å…¥ä¸Š
        seq_len = x.size(1)
        return x + self.pe[:seq_len]  # å¹¿æ’­ï¼š(batch,seq,d_model) + (seq,d_model)

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # d_model -> d_ff
        self.linear2 = nn.Linear(d_ff, d_model)  # d_ff -> d_model

    
    def forward(self, x):
        # x -> Linear1 -> ReLU -> Linear2
        return self.linear2(F.relu(self.linear1(x)))

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ff = FeedForward(d_model, d_ff)
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # Pre-LayerNorm ç‰ˆæœ¬ï¼ˆæ¨èï¼Œè®­ç»ƒæ›´ç¨³å®šï¼‰
        # Self-Attentionå­å±‚
        norm_x = self.layer_norm1(x)
        attention_output = self.mha(norm_x, norm_x, norm_x, mask)
        x = x + self.dropout(attention_output)  # æ®‹å·®è¿æ¥ + Dropout
        
        # Feed-Forwardå­å±‚  
        norm_x = self.layer_norm2(x)
        ff_output = self.ff(norm_x)
        x = x + self.dropout(ff_output)  # æ®‹å·®è¿æ¥ + Dropout
        
        return x
        
        # åŸå§‹Post-LNç‰ˆæœ¬ï¼ˆä½ çš„å®ç°ï¼Œä¹Ÿå®Œå…¨æ­£ç¡®ï¼‰ï¼š
        # attention_output = self.mha(x, x, x, mask)
        # x = self.layer_norm1(x + attention_output)
        # ff_output = self.ff(x)
        # return self.layer_norm2(ff_output + x)

class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):
        super().__init__()
        
        # 4ä¸ªæ ¸å¿ƒç»„ä»¶
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionEncoding(d_model)
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout) 
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x, mask=None):
        # x: (batch_size, seq_len) - Token IDs
        
        # 1. Token IDs -> Dense Embeddings
        x = self.embedding(x)  # (batch_size, seq_len, d_model)
        
        # 2. Add Position Encoding  
        x = self.pos_encoding(x)  # (batch_size, seq_len, d_model)
        
        # 3. Pass through all Encoder layers
        for layer in self.layers:
            x = layer(x, mask)  # (batch_size, seq_len, d_model)
        
        # 4. Final LayerNorm (for Pre-LN architecture)
        x = self.norm(x)  # (batch_size, seq_len, d_model)
        
        return x

# æµ‹è¯•ç”¨çš„ç®€å•ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•
    vocab_size = 1000
    seq_length = 10
    batch_size = 2
    d_model = 512
    num_heads = 8
    
    # æµ‹è¯•MultiHeadAttention
    print("ğŸ§ª æµ‹è¯•MultiHeadAttention...")
    mha = MultiHeadAttention(d_model, num_heads)
    
    # åˆ›å»ºéšæœºè¾“å…¥
    x = torch.randn(batch_size, seq_length, d_model)
    
    # Self-attention (Q=K=V=x)
    output = mha(x, x, x)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"âœ… å½¢çŠ¶åŒ¹é…: {output.shape == x.shape}")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in mha.parameters())
    print(f"âœ… å‚æ•°æ€»æ•°: {total_params:,}")
    
    print("ğŸ‰ MultiHeadAttentionæµ‹è¯•é€šè¿‡ï¼")
    
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•PositionEncoding...")
    pe_layer = PositionEncoding(d_model)
    
    # æµ‹è¯•ä¸åŒé•¿åº¦çš„åºåˆ—
    for test_seq_len in [5, 10]:
        test_input = torch.randn(batch_size, test_seq_len, d_model)
        pe_output = pe_layer(test_input)
        
        print(f"âœ… åºåˆ—é•¿åº¦ {test_seq_len}: è¾“å…¥{test_input.shape} -> è¾“å‡º{pe_output.shape}")
        print(f"   å½¢çŠ¶åŒ¹é…: {pe_output.shape == test_input.shape}")
        
        # æ£€æŸ¥æ˜¯å¦ç¡®å®æ·»åŠ äº†ä½ç½®ä¿¡æ¯ï¼ˆè¾“å‡ºåº”è¯¥ä¸ç­‰äºè¾“å…¥ï¼‰
        is_different = not torch.allclose(pe_output, test_input)
        print(f"   ä½ç½®ç¼–ç ç”Ÿæ•ˆ: {is_different}")
    
        print("ğŸ‰ PositionEncodingæµ‹è¯•é€šè¿‡ï¼")
    
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•FeedForward...")
    d_ff = d_model * 4  # 4å€å…³ç³»
    ff_layer = FeedForward(d_model, d_ff)
    
    test_input = torch.randn(batch_size, seq_length, d_model)
    ff_output = ff_layer(test_input)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {test_input.shape}")  
    print(f"âœ… è¾“å‡ºå½¢çŠ¶: {ff_output.shape}")
    print(f"âœ… å½¢çŠ¶åŒ¹é…: {ff_output.shape == test_input.shape}")
    print(f"âœ… d_ff = {d_ff} (d_model Ã— 4)")
    
    # æ£€æŸ¥å‚æ•°æ•°é‡
    ff_params = sum(p.numel() for p in ff_layer.parameters())
    print(f"âœ… FeedForwardå‚æ•°æ•°: {ff_params:,}")
    
    print("ğŸ‰ FeedForwardæµ‹è¯•é€šè¿‡ï¼")
    
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•EncoderLayer...")
    encoder_layer = EncoderLayer(d_model, num_heads, d_ff, dropout=0.1)
    
    test_input = torch.randn(batch_size, seq_length, d_model)
    layer_output = encoder_layer(test_input)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"âœ… è¾“å‡ºå½¢çŠ¶: {layer_output.shape}")
    print(f"âœ… å½¢çŠ¶åŒ¹é…: {layer_output.shape == test_input.shape}")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    layer_params = sum(p.numel() for p in encoder_layer.parameters())
    print(f"âœ… EncoderLayerå‚æ•°æ•°: {layer_params:,}")
    
    print("ğŸ‰ EncoderLayeræµ‹è¯•é€šè¿‡ï¼")
    
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•å®Œæ•´çš„TransformerEncoder...")
    
    # å®Œæ•´çš„Transformerå‚æ•°
    num_layers = 6  # åŸå§‹Transformerä½¿ç”¨6å±‚
    
    transformer = TransformerEncoder(
        vocab_size=vocab_size,
        d_model=d_model, 
        num_heads=num_heads,
        d_ff=d_ff,
        num_layers=num_layers,
        dropout=0.1
    )
    
    # åˆ›å»ºtoken IDè¾“å…¥ï¼ˆè€Œä¸æ˜¯embeddingï¼‰
    token_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
    
    print(f"âœ… Token IDsè¾“å…¥å½¢çŠ¶: {token_ids.shape}")
    print(f"âœ… Token IDsèŒƒå›´: [0, {vocab_size-1}]")
    
    # å®Œæ•´å‰å‘ä¼ æ’­
    final_output = transformer(token_ids)
    
    print(f"âœ… æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {final_output.shape}")
    print(f"âœ… é¢„æœŸè¾“å‡ºå½¢çŠ¶: {(batch_size, seq_length, d_model)}")
    print(f"âœ… å½¢çŠ¶åŒ¹é…: {final_output.shape == (batch_size, seq_length, d_model)}")
    
    # è®¡ç®—æ€»å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in transformer.parameters())
    print(f"âœ… æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"âœ… æ¨¡å‹å±‚æ•°: {num_layers}")
    
    print("\nğŸ‰ğŸ‰ğŸ‰ å®Œæ•´çš„Transformer Encoderå®ç°æˆåŠŸï¼ğŸ‰ğŸ‰ğŸ‰")
    print("ğŸŒŸ æ­å–œï¼ä½ å·²ç»ä»é›¶å®ç°äº†å®Œæ•´çš„Transformeræ¶æ„ï¼")
    
    # æ¶æ„æ€»ç»“
    print(f"\nğŸ“‹ æ¶æ„æ€»ç»“:")
    print(f"   ğŸ“š è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
    print(f"   ğŸ”¢ æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"   ğŸ§  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print(f"   âš¡ å‰é¦ˆç»´åº¦: {d_ff}")
    print(f"   ğŸ—ï¸  ç¼–ç å™¨å±‚æ•°: {num_layers}")
    print(f"   ğŸ’« æ€»å‚æ•°é‡: {total_params:,}")
# Transformeræ¶æ„å®ç°è¯¦è§£ ğŸ¯

è¿™æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹çš„Transformer Encoderå®Œæ•´å®ç°ï¼Œé‡‡ç”¨ç°ä»£Pre-LayerNormæ¶æ„ï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å’Œè¯¦ç»†çš„æ•™å­¦æ€§æ³¨é‡Šã€‚

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

```python
TransformerEncoder
â”œâ”€â”€ Embedding Layer          # Token IDs â†’ å¯†é›†å‘é‡
â”œâ”€â”€ Position Encoding        # æ·»åŠ ä½ç½®ä¿¡æ¯
â”œâ”€â”€ N Ã— EncoderLayer        # å †å çš„ç¼–ç å™¨å±‚
â”‚   â”œâ”€â”€ Multi-Head Attention # è‡ªæ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ Add & Norm          # æ®‹å·®è¿æ¥ + Layer Norm
â”‚   â”œâ”€â”€ Feed-Forward        # å‰é¦ˆç¥ç»ç½‘ç»œ
â”‚   â””â”€â”€ Add & Norm          # æ®‹å·®è¿æ¥ + Layer Norm
â””â”€â”€ Final Layer Norm        # Pre-LNæ¶æ„çš„æœ€ç»ˆå½’ä¸€åŒ–
```

## ğŸ“‚ æ–‡ä»¶è¯¦ç»†è¯´æ˜

### ğŸ¯ **æ ¸å¿ƒå®ç°**

#### `transformer.py` (271è¡Œ)
å®Œæ•´çš„Transformer Encoderå®ç°ï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼š

**ä¸»è¦ç±»ï¼š**
- `MultiHeadAttention` - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- `PositionEncoding` - æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç 
- `FeedForward` - ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
- `EncoderLayer` - å•ä¸ªç¼–ç å™¨å±‚
- `TransformerEncoder` - å®Œæ•´çš„å¤šå±‚ç¼–ç å™¨

### ğŸ§ª **æ¼”ç¤ºå’Œæµ‹è¯•æ–‡ä»¶**

#### `pe_demo.py`
Position Encodingçš„è¯¦ç»†æ¼”ç¤ºï¼š
- æ•°å­¦å…¬å¼è§£é‡Š
- å¯è§†åŒ–ä½ç½®ç¼–ç çŸ©é˜µ
- ä¸åŒå‚æ•°çš„å½±å“åˆ†æ

#### `power_comparison.py`
æ•°å€¼è®¡ç®—æ–¹æ³•æ¯”è¾ƒï¼š
- `torch.pow()` vs `torch.exp(torch.log())`
- æ•°å€¼ç¨³å®šæ€§åˆ†æ
- ä¸ºä»€ä¹ˆé€‰æ‹©exp(log())æ–¹æ³•

#### `test_contiguous.py` & `test_contiguous_cases.py`
PyTorchå†…å­˜å¸ƒå±€æ·±åº¦è§£æï¼š
- `.contiguous()`çš„å¿…è¦æ€§
- transposeæ“ä½œçš„å†…å­˜å½±å“
- ä½•æ—¶éœ€è¦ä½¿ç”¨contiguous()

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. **MultiHeadAttention** ğŸ§ 

**æ ¸å¿ƒæ€æƒ³ï¼š** å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›"è§†è§’"ï¼Œæ•æ‰ä¸åŒç±»å‹çš„è¯­ä¹‰å…³ç³»ã€‚

```python
# å…³é”®å‚æ•°
d_model = 512     # æ¨¡å‹ç»´åº¦
num_heads = 8     # æ³¨æ„åŠ›å¤´æ•°
d_k = d_model // num_heads = 64  # æ¯ä¸ªå¤´çš„ç»´åº¦
```

**å®ç°è¦ç‚¹ï¼š**
- âœ… **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**ï¼š`Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`
- âœ… **å¤šå¤´å¹¶è¡Œè®¡ç®—**ï¼šæ‰€æœ‰å¤´åŒæ—¶å¤„ç†ï¼Œæœ€åæ‹¼æ¥
- âœ… **æ­£ç¡®çš„ç»´åº¦å˜æ¢**ï¼š`(batch, seq, d_model) â†’ (batch, heads, seq, d_k)`
- âœ… **contiguous()ä½¿ç”¨**ï¼šåœ¨reshapeå‰ç¡®ä¿å†…å­˜è¿ç»­æ€§

**å…³é”®ä»£ç ç‰‡æ®µï¼š**
```python
# ç”ŸæˆQã€Kã€V
Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

# è®¡ç®—æ³¨æ„åŠ›
scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)
attention_weights = F.softmax(scores, dim=-1)
attention_output = attention_weights @ V

# åˆå¹¶å¤šå¤´ç»“æœ
attention_output = attention_output.transpose(1, 2).contiguous().view(
    batch_size, seq_len, self.d_model
)
```

### 2. **PositionEncoding** ğŸ“

**æ ¸å¿ƒæ€æƒ³ï¼š** ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®æ·»åŠ å”¯ä¸€çš„ä½ç½®ä¿¡æ¯ã€‚

**æ•°å­¦å…¬å¼ï¼š**
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**å®ç°è¦ç‚¹ï¼š**
- âœ… **å›ºå®šç¼–ç **ï¼šä¸å¯å­¦ä¹ ï¼Œä½†å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§
- âœ… **ç›¸å¯¹ä½ç½®æ„ŸçŸ¥**ï¼šèƒ½å¤Ÿæ„ŸçŸ¥è¯æ±‡é—´çš„ç›¸å¯¹è·ç¦»
- âœ… **ä»»æ„é•¿åº¦æ”¯æŒ**ï¼šç†è®ºä¸Šæ”¯æŒä»»æ„åºåˆ—é•¿åº¦
- âœ… **register_buffer**ï¼šç¡®ä¿GPUå…¼å®¹æ€§

**ä¸ºä»€ä¹ˆç”¨exp(log())è€Œä¸æ˜¯pow()ï¼Ÿ**
```python
# æ•°å€¼ç¨³å®šçš„è®¡ç®—æ–¹å¼
div_term = torch.exp(i * (-math.log(10000.0) / d_model))

# è€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ï¼ˆè™½ç„¶æ•°å­¦ä¸Šç­‰ä»·ï¼‰
div_term = torch.pow(10000.0, -2*i/d_model)
```

### 3. **FeedForward** âš¡

**æ ¸å¿ƒæ€æƒ³ï¼š** ç®€å•ä½†æœ‰æ•ˆçš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œæä¾›éçº¿æ€§å˜æ¢ã€‚

**æ¶æ„ï¼š** `Linear(d_model â†’ d_ff) â†’ ReLU â†’ Linear(d_ff â†’ d_model)`

**å®ç°è¦ç‚¹ï¼š**
- âœ… **4å€æ‰©å±•**ï¼šé€šå¸¸`d_ff = 4 Ã— d_model`
- âœ… **ReLUæ¿€æ´»**ï¼šå¼•å…¥éçº¿æ€§
- âœ… **ç»´åº¦æ¢å¤**ï¼šè¾“å‡ºç»´åº¦ä¸è¾“å…¥ç›¸åŒ

```python
def forward(self, x):
    return self.linear2(F.relu(self.linear1(x)))
```

### 4. **EncoderLayer** ğŸ”—

**æ ¸å¿ƒæ€æƒ³ï¼š** ç»„åˆæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œï¼Œä½¿ç”¨Pre-LayerNormæ¶æ„ã€‚

**Pre-LN vs Post-LNï¼š**
```python
# Pre-LN (æˆ‘ä»¬çš„å®ç°ï¼Œæ›´ç¨³å®š)
x = x + sublayer(LayerNorm(x))

# Post-LN (åŸå§‹è®ºæ–‡ï¼Œä½†è®­ç»ƒå¯èƒ½ä¸ç¨³å®š)
x = LayerNorm(x + sublayer(x))
```

**å®ç°è¦ç‚¹ï¼š**
- âœ… **Pre-LayerNorm**ï¼šç°ä»£Transformerçš„æ ‡å‡†åšæ³•
- âœ… **æ®‹å·®è¿æ¥**ï¼šè§£å†³æ·±åº¦ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- âœ… **Dropoutæ­£åˆ™åŒ–**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

### 5. **TransformerEncoder** ğŸš€

**æ ¸å¿ƒæ€æƒ³ï¼š** å®Œæ•´çš„ç¼–ç å™¨ï¼Œä»Token IDsåˆ°ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚

**æ•°æ®æµï¼š**
```
Token IDs (batch, seq) 
    â†“ Embedding
Dense Vectors (batch, seq, d_model)
    â†“ Position Encoding
Enhanced Vectors (batch, seq, d_model)
    â†“ N Ã— EncoderLayer  
Contextualized Vectors (batch, seq, d_model)
    â†“ Final LayerNorm
Output (batch, seq, d_model)
```

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### ğŸ”´ **å¸¸è§é™·é˜±**

#### 1. **contiguous()çš„ä½¿ç”¨**
```python
# âŒ é”™è¯¯ï¼štransposeåç›´æ¥viewå¯èƒ½å¤±è´¥
x = x.transpose(1, 2).view(new_shape)

# âœ… æ­£ç¡®ï¼šç¡®ä¿å†…å­˜è¿ç»­æ€§
x = x.transpose(1, 2).contiguous().view(new_shape)
```

#### 2. **ç»´åº¦åŒ¹é…**
```python
# ç¡®ä¿è¿™äº›ç»´åº¦èƒ½æ•´é™¤
assert d_model % num_heads == 0
```

#### 3. **maskçš„æ­£ç¡®åº”ç”¨**
```python
# maskåº”è¯¥åœ¨softmaxä¹‹å‰åº”ç”¨
if mask is not None:
    scores = scores.masked_fill(mask == 0, -1e9)
attention_weights = F.softmax(scores, dim=-1)
```

### ğŸŸ¡ **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

#### 1. **å†…å­˜æ•ˆç‡**
- ä½¿ç”¨`register_buffer`è€Œä¸æ˜¯`Parameter`å­˜å‚¨Position Encoding
- é¢„è®¡ç®—å¹¶ç¼“å­˜å¸¸ç”¨çš„maskçŸ©é˜µ

#### 2. **è®¡ç®—æ•ˆç‡**
- æ‰¹å¤„ç†è¾“å…¥ä»¥å……åˆ†åˆ©ç”¨GPUå¹¶è¡Œæ€§
- è€ƒè™‘ä½¿ç”¨Flash Attentionç­‰ä¼˜åŒ–å®ç°

#### 3. **æ•°å€¼ç¨³å®šæ€§**
- ä½¿ç”¨é€‚å½“çš„åˆå§‹åŒ–æ–¹æ³•
- ç›‘æ§æ¢¯åº¦èŒƒæ•°ï¼Œå¿…è¦æ—¶ä½¿ç”¨æ¢¯åº¦è£å‰ª

### ğŸŸ¢ **æ‰©å±•å»ºè®®**

#### 1. **åŠŸèƒ½æ‰©å±•**
```python
# æ·»åŠ æ›´å¤šä½ç½®ç¼–ç æ–¹å¼
class LearnablePositionEncoding(nn.Module): ...

# å®ç°ç›¸å¯¹ä½ç½®ç¼–ç 
class RelativePositionEncoding(nn.Module): ...

# æ·»åŠ ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶
class SparseAttention(nn.Module): ...
```

#### 2. **è®­ç»ƒæŠ€å·§**
- å­¦ä¹ ç‡é¢„çƒ­ (Learning Rate Warmup)
- æ ‡ç­¾å¹³æ»‘ (Label Smoothing)
- æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### **è¿è¡Œæµ‹è¯•**
```bash
cd transformer/
python transformer.py  # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
```

**æµ‹è¯•åŒ…å«ï¼š**
- âœ… MultiHeadAttentionå½¢çŠ¶éªŒè¯
- âœ… PositionEncodingæ•°å€¼æµ‹è¯•
- âœ… FeedForwardåŠŸèƒ½æµ‹è¯•
- âœ… EncoderLayeré›†æˆæµ‹è¯•
- âœ… TransformerEncoderç«¯åˆ°ç«¯æµ‹è¯•

### **æ€§èƒ½åŸºå‡†**
åœ¨æ ‡å‡†é…ç½®ä¸‹çš„å‚æ•°ç»Ÿè®¡ï¼š
```
é…ç½®: vocab_size=1000, d_model=512, num_heads=8, d_ff=2048, num_layers=6
æ€»å‚æ•°é‡: ~25M (ä¸»è¦åœ¨Embeddingå±‚)
å†…å­˜å ç”¨: ~100MB (FP32)
```

## ğŸ” è°ƒè¯•æŠ€å·§

### **å¸¸è§é—®é¢˜æ’æŸ¥**

#### 1. **å½¢çŠ¶ä¸åŒ¹é…**
```python
# åœ¨å…³é”®ä½ç½®æ·»åŠ å½¢çŠ¶æ£€æŸ¥
print(f"Q shape: {Q.shape}")  # åº”è¯¥æ˜¯ (batch, heads, seq, d_k)
print(f"K shape: {K.shape}")  # åº”è¯¥æ˜¯ (batch, heads, seq, d_k)
print(f"V shape: {V.shape}")  # åº”è¯¥æ˜¯ (batch, heads, seq, d_k)
```

#### 2. **æ³¨æ„åŠ›æƒé‡å¼‚å¸¸**
```python
# æ£€æŸ¥æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
print(f"Attention weights sum: {attention_weights.sum(dim=-1)}")  # åº”è¯¥éƒ½æ˜¯1
print(f"Attention weights range: {attention_weights.min()} to {attention_weights.max()}")
```

#### 3. **æ¢¯åº¦é—®é¢˜**
```python
# æ£€æŸ¥æ¢¯åº¦æµ
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm()}")
```

## ğŸ“š å­¦ä¹ èµ„æº

### **æ¨èé˜…è¯»é¡ºåº**
1. ğŸ“– å…ˆç†è§£æ¯ä¸ªç»„ä»¶çš„æ•°å­¦åŸç†
2. ğŸ” é€è¡Œé˜…è¯»ä»£ç å®ç°
3. ğŸ§ª è¿è¡Œæ¼”ç¤ºè„šæœ¬åŠ æ·±ç†è§£
4. ğŸ”§ å°è¯•ä¿®æ”¹å‚æ•°è§‚å¯Ÿæ•ˆæœ
5. ğŸš€ æ‰©å±•å®ç°æ·»åŠ æ–°åŠŸèƒ½

### **æ·±å…¥å­¦ä¹ **
- **æ³¨æ„åŠ›æœºåˆ¶å˜ä½“**ï¼šSparse Attention, Linear Attention
- **ä½ç½®ç¼–ç æ”¹è¿›**ï¼šRoPE, ALiBiç­‰
- **æ¶æ„ä¼˜åŒ–**ï¼šT5, Switch Transformerç­‰
- **è®­ç»ƒæŠ€æœ¯**ï¼šæ··åˆç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒ

---

## ğŸŠ å®ç°äº®ç‚¹æ€»ç»“

- ğŸ”¥ **ç°ä»£Pre-LNæ¶æ„**ï¼šè®­ç»ƒæ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«
- ğŸ”¥ **å®Œæ•´ç»´åº¦æ ‡æ³¨**ï¼šæ¯ä¸ªå¼ é‡å˜æ¢éƒ½æœ‰è¯¦ç»†æ³¨é‡Š
- ğŸ”¥ **æ•™å­¦å‹å¥½è®¾è®¡**ï¼šä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
- ğŸ”¥ **å·¥ç¨‹å®è·µè§„èŒƒ**ï¼šéµå¾ªPyTorchæœ€ä½³å®è·µ
- ğŸ”¥ **å¯æ‰©å±•æ¶æ„**ï¼šæ˜“äºæ·»åŠ æ–°åŠŸèƒ½å’Œæ”¹è¿›

**è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå®ç°ï¼Œæ›´æ˜¯ä¸€ä¸ªå­¦ä¹ Transformerçš„å®Œæ•´æ•™ç¨‹ï¼** ğŸ“–âœ¨
{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T13:31:40.595604Z",
     "start_time": "2025-07-30T13:31:19.310766Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\", cache_dir=\"K:\\hugging-hub-cache\")\n",
    "raw_datasets"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\jhyan\\AppData\\Local\\Temp\\ipykernel_38544\\3693815070.py:3: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  raw_datasets = load_dataset(\"glue\", \"mrpc\", cache_dir=\"K:\\hugging-hub-cache\")\n",
      "C:\\Users\\jhyan\\.conda\\envs\\llm-learn\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "C:\\Users\\jhyan\\.conda\\envs\\llm-learn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 457605.80 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 203796.12 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 428624.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T15:15:49.004655Z",
     "start_time": "2025-07-30T15:15:48.948576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_datasets[\"train\"][\"sentence1\"]\n",
    "print(\"------------\")\n",
    "len(list(raw_datasets[\"train\"][\"sentence2\"]))"
   ],
   "id": "5a9119e9f91937ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:05:24.869448Z",
     "start_time": "2025-07-30T16:05:23.061873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_dataset = tokenizer(\n",
    "    list(raw_datasets[\"train\"][\"sentence1\"]),\n",
    "    list(raw_datasets[\"train\"][\"sentence2\"]),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "type(tokenized_dataset)"
   ],
   "id": "87d32e63e7507c56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:07:44.561875Z",
     "start_time": "2025-07-30T16:07:44.559169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"],\n",
    "                     truncation=True)"
   ],
   "id": "a243fb0bcbcb536",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:07:47.006447Z",
     "start_time": "2025-07-30T16:07:46.724836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ],
   "id": "6fbd0cd72de3a879",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:00<00:00, 23918.56 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 13350.57 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 22236.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:08:19.458973Z",
     "start_time": "2025-07-30T16:08:19.425888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "id": "7d1fb192181937ea",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:10:20.400535Z",
     "start_time": "2025-07-30T16:10:20.394988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "samples = tokenized_dataset[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]\n"
   ],
   "id": "28bc740e4fc8eb40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:11:02.779947Z",
     "start_time": "2025-07-30T16:11:02.774739Z"
    }
   },
   "cell_type": "code",
   "source": "batch = data_collator(samples)",
   "id": "96ccb26a15ac3562",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T16:11:04.348761Z",
     "start_time": "2025-07-30T16:11:04.344582Z"
    }
   },
   "cell_type": "code",
   "source": "{k: v.shape for k, v in batch.items()}",
   "id": "64b9805cc70aa0c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cace79fed9cfe666"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

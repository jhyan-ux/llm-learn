{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-05T08:55:47.722295Z",
     "start_time": "2025-08-05T08:55:42.892263Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('code_search_net', 'python', trust_remote_code=True)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:56:39.920530Z",
     "start_time": "2025-08-05T08:56:39.917488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ],
   "id": "93438ac38f62595a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:58:21.582875Z",
     "start_time": "2025-08-05T08:58:20.494158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ],
   "id": "dd525f8508da5e60",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:00:05.640227Z",
     "start_time": "2025-08-05T09:00:05.635053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ],
   "id": "9a88c4cdc47ede3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:01:07.314362Z",
     "start_time": "2025-08-05T09:01:07.310757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ],
   "id": "9d504b2094d2d444",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:02:28.853401Z",
     "start_time": "2025-08-05T09:01:21.953972Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)",
   "id": "7841cc74e34ac52c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:05:25.837119Z",
     "start_time": "2025-08-05T09:05:25.831954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ],
   "id": "1d508e885663a69d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:06:20.850778Z",
     "start_time": "2025-08-05T09:06:20.838449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "4a95765f591e507a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d63bdc71455445c989cedc6f2f7e286a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:08:03.369362Z",
     "start_time": "2025-08-05T09:07:58.808278Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.push_to_hub(\"code-search-net-tokenizer\")",
   "id": "2c188785af8d1aed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jhyannnn/code-search-net-tokenizer/commit/c13628b8d18c41d12e1c050e98e8210d6e58618a', commit_message='Upload tokenizer', commit_description='', oid='c13628b8d18c41d12e1c050e98e8210d6e58618a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/jhyannnn/code-search-net-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='jhyannnn/code-search-net-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:16:53.219097Z",
     "start_time": "2025-08-05T09:16:53.214559Z"
    }
   },
   "cell_type": "code",
   "source": "type(tokens)",
   "id": "ad77f050e5da9829",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:16:24.734607Z",
     "start_time": "2025-08-05T09:16:24.708446Z"
    }
   },
   "cell_type": "code",
   "source": "from",
   "id": "b97c42626ab22ccb",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'word_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m tokens.word_ids()\n",
      "\u001B[31mAttributeError\u001B[39m: 'list' object has no attribute 'word_ids'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:54:48.602429Z",
     "start_time": "2025-08-05T09:54:47.628919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ],
   "id": "1490e5c5800983",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:56:52.853305Z",
     "start_time": "2025-08-05T09:56:52.849748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = \"\"\"\n",
    "正如我们将在下一节中看到的，tokenizer 一般不会在原始文本上进行训练。    因此，我们首先需要将文本拆分为更小的实体，例如单词。这就是预分词步骤的作用。正如我们在 第二章 中看到的，基于单词的 tokenizer 可以简单地根据空格和标点符号将原始文本拆分为单词。这些词将是 tokenizer 在训练期间可以学习的子词的边界。\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer.tokenize(example)"
   ],
   "id": "ed3a0c30e69c325",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:56:54.169827Z",
     "start_time": "2025-08-05T09:56:54.163161Z"
    }
   },
   "cell_type": "code",
   "source": "tokens",
   "id": "8f936b2b3ae37b2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['正',\n",
       " '如',\n",
       " '我',\n",
       " '们',\n",
       " '将',\n",
       " '在',\n",
       " '下',\n",
       " '一',\n",
       " '节',\n",
       " '中',\n",
       " '看',\n",
       " '到',\n",
       " '的',\n",
       " '，',\n",
       " 'to',\n",
       " '##ken',\n",
       " '##ize',\n",
       " '##r',\n",
       " '一',\n",
       " '般',\n",
       " '不',\n",
       " '会',\n",
       " '在',\n",
       " '原',\n",
       " '始',\n",
       " '文',\n",
       " '本',\n",
       " '上',\n",
       " '进',\n",
       " '行',\n",
       " '训',\n",
       " '练',\n",
       " '。',\n",
       " '因',\n",
       " '此',\n",
       " '，',\n",
       " '我',\n",
       " '们',\n",
       " '首',\n",
       " '先',\n",
       " '需',\n",
       " '要',\n",
       " '将',\n",
       " '文',\n",
       " '本',\n",
       " '拆',\n",
       " '分',\n",
       " '为',\n",
       " '更',\n",
       " '小',\n",
       " '的',\n",
       " '实',\n",
       " '体',\n",
       " '，',\n",
       " '例',\n",
       " '如',\n",
       " '单',\n",
       " '词',\n",
       " '。',\n",
       " '这',\n",
       " '就',\n",
       " '是',\n",
       " '预',\n",
       " '分',\n",
       " '词',\n",
       " '步',\n",
       " '骤',\n",
       " '的',\n",
       " '作',\n",
       " '用',\n",
       " '。',\n",
       " '正',\n",
       " '如',\n",
       " '我',\n",
       " '们',\n",
       " '在',\n",
       " '第',\n",
       " '二',\n",
       " '章',\n",
       " '中',\n",
       " '看',\n",
       " '到',\n",
       " '的',\n",
       " '，',\n",
       " '基',\n",
       " '于',\n",
       " '单',\n",
       " '词',\n",
       " '的',\n",
       " 'to',\n",
       " '##ken',\n",
       " '##ize',\n",
       " '##r',\n",
       " '可',\n",
       " '以',\n",
       " '简',\n",
       " '单',\n",
       " '地',\n",
       " '根',\n",
       " '据',\n",
       " '空',\n",
       " '格',\n",
       " '和',\n",
       " '标',\n",
       " '点',\n",
       " '符',\n",
       " '号',\n",
       " '将',\n",
       " '原',\n",
       " '始',\n",
       " '文',\n",
       " '本',\n",
       " '拆',\n",
       " '分',\n",
       " '为',\n",
       " '单',\n",
       " '词',\n",
       " '。',\n",
       " '这',\n",
       " '些',\n",
       " '词',\n",
       " '将',\n",
       " '是',\n",
       " 'to',\n",
       " '##ken',\n",
       " '##ize',\n",
       " '##r',\n",
       " '在',\n",
       " '训',\n",
       " '练',\n",
       " '期',\n",
       " '间',\n",
       " '可',\n",
       " '以',\n",
       " '学',\n",
       " '习',\n",
       " '的',\n",
       " '子',\n",
       " '词',\n",
       " '的',\n",
       " '边',\n",
       " '界',\n",
       " '。']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:57:34.788197Z",
     "start_time": "2025-08-05T09:57:34.782114Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(example)",
   "id": "e28a45eb262ee23d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('正如我们将在下一节中看到的', (1, 14)),\n",
       " ('，', (14, 15)),\n",
       " ('tokenizer', (15, 24)),\n",
       " ('一般不会在原始文本上进行训练', (25, 39)),\n",
       " ('。', (39, 40)),\n",
       " ('因此', (44, 46)),\n",
       " ('，', (46, 47)),\n",
       " ('我们首先需要将文本拆分为更小的实体', (47, 64)),\n",
       " ('，', (64, 65)),\n",
       " ('例如单词', (65, 69)),\n",
       " ('。', (69, 70)),\n",
       " ('这就是预分词步骤的作用', (70, 81)),\n",
       " ('。', (81, 82)),\n",
       " ('正如我们在', (82, 87)),\n",
       " ('第二章', (88, 91)),\n",
       " ('中看到的', (92, 96)),\n",
       " ('，', (96, 97)),\n",
       " ('基于单词的', (97, 102)),\n",
       " ('tokenizer', (103, 112)),\n",
       " ('可以简单地根据空格和标点符号将原始文本拆分为单词', (113, 137)),\n",
       " ('。', (137, 138)),\n",
       " ('这些词将是', (138, 143)),\n",
       " ('tokenizer', (144, 153)),\n",
       " ('在训练期间可以学习的子词的边界', (154, 169)),\n",
       " ('。', (169, 170))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb91b0f9c9665abe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

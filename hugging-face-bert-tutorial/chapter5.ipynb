{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.251115Z",
     "start_time": "2025-08-03T12:03:30.619772Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from mpmath.identification import transforms\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./drugsComTest_raw.tsv\",\n",
    "    \"test\": \"./drugsComTrain_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "drug_dataset"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhyan\\.conda\\envs\\llm-learn\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.293956Z",
     "start_time": "2025-08-03T12:03:34.270929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]\n"
   ],
   "id": "66d835eb36120788",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [184648, 25268, 172019],\n",
       " 'drugName': ['Efudex', 'Flector Patch', 'Amitiza'],\n",
       " 'condition': ['Basal Cell Carcinoma', 'Pain', 'Irritable Bowel Syndrome'],\n",
       " 'review': ['\"I have BCC on my upper arm and SCC on upper left hand. Unfortunately after 6wks of treatment twice a day the cream didnt work. So disappointed and im now scheduled to have both surgically removed.\"',\n",
       "  '\"I tore my shoulder labrum and the pain can be off the chart.  Hydrocodone and ibuprofen and ice helped some. After my doctor gave me the Flector Patch I noticed major relief in my shoulder within an hour. These work very well. These truly work.\"',\n",
       "  '\"Amitiza is the best if you have ibs!\"'],\n",
       " 'rating': [1.0, 8.0, 10.0],\n",
       " 'date': ['August 30, 2016', 'May 29, 2014', 'July 13, 2016'],\n",
       " 'usefulCount': [16, 40, 9]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.839197Z",
     "start_time": "2025-08-03T12:03:34.832016Z"
    }
   },
   "cell_type": "code",
   "source": "drug_dataset = drug_dataset.rename_column(\"Unnamed: 0\", \"patient_id\")",
   "id": "1a34c581fd57eee4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.891951Z",
     "start_time": "2025-08-03T12:03:34.850365Z"
    }
   },
   "cell_type": "code",
   "source": "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)",
   "id": "c8de30ccfe472a2f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:35.104366Z",
     "start_time": "2025-08-03T12:03:35.027481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lowercase_condition(examples):\n",
    "    return {\"condition\": examples[\"condition\"].lower()}\n",
    "\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)"
   ],
   "id": "2d18fa1b053b342f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:51.152551Z",
     "start_time": "2025-08-03T12:03:35.114407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_review_length(examples):\n",
    "    return {\"review_length\": len(examples[\"review\"].split())}\n",
    "\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)"
   ],
   "id": "b785b4d8da6ad3ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a82fbb27916147869c3c5398d5a7b1c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90adabd63b344efe83a31358d8ef100c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:52.897386Z",
     "start_time": "2025-08-03T12:03:51.160895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ],
   "id": "ad6b6872cba11bab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c4169918bf84e019e39c03a31b1e802"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bb8a1d7eb9f4c53a6250473f1acb24d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 46108, 'test': 138514}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:00.993958Z",
     "start_time": "2025-08-03T12:03:52.910005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import html\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True)"
   ],
   "id": "ca07607d74e7d4ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51a97c95e12e43e8b3c96dd312adbb4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a79581da0523468c86cb42825bcee962"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.006278Z",
     "start_time": "2025-08-03T12:04:01.003138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 更改数据格式\n",
    "drug_dataset.set_format(type=\"pandas\")"
   ],
   "id": "4cc261e0738048c6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.019573Z",
     "start_time": "2025-08-03T12:04:01.016241Z"
    }
   },
   "cell_type": "code",
   "source": "type(drug_dataset)",
   "id": "c5cff9ef6a042c46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.049020Z",
     "start_time": "2025-08-03T12:04:01.046522Z"
    }
   },
   "cell_type": "code",
   "source": "drug_dataset[\"train\"].set_format(type=\"pandas\")",
   "id": "cb56fac48efb5495",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:15:43.018406Z",
     "start_time": "2025-08-03T12:15:42.408651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = list(np.unique(drug_dataset[\"train\"][\"condition\"]))\n",
    "labels = [i for i in all_labels if \"span\" not in i]\n",
    "filter_labels = [i for i in all_labels if \"span\" in i]\n",
    "len(labels)"
   ],
   "id": "4e40dfa3709b849e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.103379Z",
     "start_time": "2025-08-03T12:04:01.099355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drug_dataset.reset_format()\n",
    "type(drug_dataset[\"test\"])"
   ],
   "id": "d2bd3817e2dde39e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:18:19.317926Z",
     "start_time": "2025-08-03T12:18:17.744256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shuffle_dataset = drug_dataset[\"train\"].train_test_split(test_size=0.2, train_size=0.8, seed=42)\n",
    "\n",
    "\n",
    "def filter_condition(example):\n",
    "    if example[\"condition\"] in filter_labels:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "shuffle_dataset = shuffle_dataset.filter(filter_condition)"
   ],
   "id": "cb1199e590e8f9a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/36886 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2eeb8dcdec004a37bb5abea4224589df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/9222 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "372bdd36c49a46e0b312a9ba27fd90f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:18:35.631366Z",
     "start_time": "2025-08-03T12:18:35.627861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label2Id = {label: i for i, label in enumerate(labels)}\n",
    "id2Label = {i: label for i, label in enumerate(labels)}"
   ],
   "id": "53e710780c61cc8d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:30:11.835534Z",
     "start_time": "2025-08-03T12:30:10.915122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"review\"], truncation=True)\n",
    "    tokenized_inputs[\"labels\"] = [label2Id[i] for i in examples[\"condition\"]]\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = shuffle_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([o for o in tokenized_datasets.column_names[\"train\"] if o not in [\"labels\", \"review\", \"patient_id\", \"input_ids\",\"attention_mask\", \"token_type_ids\"]])"
   ],
   "id": "42479b14e86836fa",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:52:21.023076Z",
     "start_time": "2025-08-03T12:52:20.895848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 提取样本列表（如取前 4 个样本）\n",
    "samples = tokenized_datasets[\"train\"][: 4]\n",
    "\n",
    "# 生成 Batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = data_collator(samples)\n",
    "print(batch)"
   ],
   "id": "c7115ae270ee4069",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`review` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:767\u001B[39m, in \u001B[36mBatchEncoding.convert_to_tensors\u001B[39m\u001B[34m(self, tensor_type, prepend_batch_axis)\u001B[39m\n\u001B[32m    766\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor(value):\n\u001B[32m--> \u001B[39m\u001B[32m767\u001B[39m     tensor = as_tensor(value)\n\u001B[32m    769\u001B[39m     \u001B[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001B[39;00m\n\u001B[32m    770\u001B[39m     \u001B[38;5;66;03m# # at-least2d\u001B[39;00m\n\u001B[32m    771\u001B[39m     \u001B[38;5;66;03m# if tensor.ndim > 2:\u001B[39;00m\n\u001B[32m    772\u001B[39m     \u001B[38;5;66;03m#     tensor = tensor.squeeze(0)\u001B[39;00m\n\u001B[32m    773\u001B[39m     \u001B[38;5;66;03m# elif tensor.ndim < 2:\u001B[39;00m\n\u001B[32m    774\u001B[39m     \u001B[38;5;66;03m#     tensor = tensor[None, :]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:729\u001B[39m, in \u001B[36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001B[39m\u001B[34m(value, dtype)\u001B[39m\n\u001B[32m    728\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m torch.from_numpy(np.array(value))\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m torch.tensor(value)\n",
      "\u001B[31mValueError\u001B[39m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# 生成 Batch\u001B[39;00m\n\u001B[32m      5\u001B[39m data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m batch = data_collator(samples)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(batch)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\data\\data_collator.py:272\u001B[39m, in \u001B[36mDataCollatorWithPadding.__call__\u001B[39m\u001B[34m(self, features)\u001B[39m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[32m--> \u001B[39m\u001B[32m272\u001B[39m     batch = pad_without_fast_tokenizer_warning(\n\u001B[32m    273\u001B[39m         \u001B[38;5;28mself\u001B[39m.tokenizer,\n\u001B[32m    274\u001B[39m         features,\n\u001B[32m    275\u001B[39m         padding=\u001B[38;5;28mself\u001B[39m.padding,\n\u001B[32m    276\u001B[39m         max_length=\u001B[38;5;28mself\u001B[39m.max_length,\n\u001B[32m    277\u001B[39m         pad_to_multiple_of=\u001B[38;5;28mself\u001B[39m.pad_to_multiple_of,\n\u001B[32m    278\u001B[39m         return_tensors=\u001B[38;5;28mself\u001B[39m.return_tensors,\n\u001B[32m    279\u001B[39m     )\n\u001B[32m    280\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[32m    281\u001B[39m         batch[\u001B[33m\"\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m\"\u001B[39m] = batch[\u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\data\\data_collator.py:67\u001B[39m, in \u001B[36mpad_without_fast_tokenizer_warning\u001B[39m\u001B[34m(tokenizer, *pad_args, **pad_kwargs)\u001B[39m\n\u001B[32m     64\u001B[39m tokenizer.deprecation_warnings[\u001B[33m\"\u001B[39m\u001B[33mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     padded = tokenizer.pad(*pad_args, **pad_kwargs)\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     69\u001B[39m     \u001B[38;5;66;03m# Restore the state of the warning.\u001B[39;00m\n\u001B[32m     70\u001B[39m     tokenizer.deprecation_warnings[\u001B[33m\"\u001B[39m\u001B[33mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[33m\"\u001B[39m] = warning_state\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3373\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.pad\u001B[39m\u001B[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001B[39m\n\u001B[32m   3370\u001B[39m             batch_outputs[key] = []\n\u001B[32m   3371\u001B[39m         batch_outputs[key].append(value)\n\u001B[32m-> \u001B[39m\u001B[32m3373\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:240\u001B[39m, in \u001B[36mBatchEncoding.__init__\u001B[39m\u001B[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001B[39m\n\u001B[32m    236\u001B[39m     n_sequences = encoding[\u001B[32m0\u001B[39m].n_sequences\n\u001B[32m    238\u001B[39m \u001B[38;5;28mself\u001B[39m._n_sequences = n_sequences\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m \u001B[38;5;28mself\u001B[39m.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:783\u001B[39m, in \u001B[36mBatchEncoding.convert_to_tensors\u001B[39m\u001B[34m(self, tensor_type, prepend_batch_axis)\u001B[39m\n\u001B[32m    778\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m key == \u001B[33m\"\u001B[39m\u001B[33moverflowing_tokens\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    779\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    780\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mUnable to create tensor returning overflowing tokens of different lengths. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    781\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mPlease see if a fast version of this tokenizer is available to have this feature available.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    782\u001B[39m             ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m783\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    784\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mUnable to create tensor, you should probably activate truncation and/or padding with\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    785\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33mpadding=True\u001B[39m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m\u001B[33mtruncation=True\u001B[39m\u001B[33m'\u001B[39m\u001B[33m to have batched tensors with the same length. Perhaps your\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    786\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m features (`\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    787\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m expected).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    788\u001B[39m         ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    790\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[31mValueError\u001B[39m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`review` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.940110400Z",
     "start_time": "2025-08-03T11:58:12.685014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels))\n",
    "\n",
    "data_collator(tokenized_datasets[\"train\"][:10])"
   ],
   "id": "6430ec35d960ecd0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[100]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoModelForSequenceClassification\n\u001B[32m      3\u001B[39m model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=\u001B[38;5;28mlen\u001B[39m(labels))\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m data_collator(tokenized_datasets[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\data\\data_collator.py:272\u001B[39m, in \u001B[36mDataCollatorWithPadding.__call__\u001B[39m\u001B[34m(self, features)\u001B[39m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[32m--> \u001B[39m\u001B[32m272\u001B[39m     batch = pad_without_fast_tokenizer_warning(\n\u001B[32m    273\u001B[39m         \u001B[38;5;28mself\u001B[39m.tokenizer,\n\u001B[32m    274\u001B[39m         features,\n\u001B[32m    275\u001B[39m         padding=\u001B[38;5;28mself\u001B[39m.padding,\n\u001B[32m    276\u001B[39m         max_length=\u001B[38;5;28mself\u001B[39m.max_length,\n\u001B[32m    277\u001B[39m         pad_to_multiple_of=\u001B[38;5;28mself\u001B[39m.pad_to_multiple_of,\n\u001B[32m    278\u001B[39m         return_tensors=\u001B[38;5;28mself\u001B[39m.return_tensors,\n\u001B[32m    279\u001B[39m     )\n\u001B[32m    280\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[32m    281\u001B[39m         batch[\u001B[33m\"\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m\"\u001B[39m] = batch[\u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\data\\data_collator.py:67\u001B[39m, in \u001B[36mpad_without_fast_tokenizer_warning\u001B[39m\u001B[34m(tokenizer, *pad_args, **pad_kwargs)\u001B[39m\n\u001B[32m     64\u001B[39m tokenizer.deprecation_warnings[\u001B[33m\"\u001B[39m\u001B[33mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     padded = tokenizer.pad(*pad_args, **pad_kwargs)\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     69\u001B[39m     \u001B[38;5;66;03m# Restore the state of the warning.\u001B[39;00m\n\u001B[32m     70\u001B[39m     tokenizer.deprecation_warnings[\u001B[33m\"\u001B[39m\u001B[33mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[33m\"\u001B[39m] = warning_state\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3292\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.pad\u001B[39m\u001B[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001B[39m\n\u001B[32m   3288\u001B[39m \u001B[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001B[39;00m\n\u001B[32m   3289\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model_input_names[\u001B[32m0\u001B[39m] \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m encoded_inputs:\n\u001B[32m   3290\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   3291\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou should supply an encoding or a list of encodings to this method \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m3292\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mthat includes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.model_input_names[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, but you provided \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(encoded_inputs.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   3293\u001B[39m     )\n\u001B[32m   3295\u001B[39m required_input = encoded_inputs[\u001B[38;5;28mself\u001B[39m.model_input_names[\u001B[32m0\u001B[39m]]\n\u001B[32m   3297\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m required_input \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(required_input, Sized) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(required_input) == \u001B[32m0\u001B[39m):\n",
      "\u001B[31mAttributeError\u001B[39m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.941111600Z",
     "start_time": "2025-08-03T11:43:30.371455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./train-test\", eval_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")"
   ],
   "id": "79313df12c06ab46",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.942614200Z",
     "start_time": "2025-08-03T11:45:19.676168Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "84e8d2f00e6a3c18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhyan\\.conda\\envs\\llm-learn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[84]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m trainer.train()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\trainer.py:2237\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2235\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2236\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2237\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[32m   2238\u001B[39m         args=args,\n\u001B[32m   2239\u001B[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001B[32m   2240\u001B[39m         trial=trial,\n\u001B[32m   2241\u001B[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001B[32m   2242\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\trainer.py:2578\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2571\u001B[39m context = (\n\u001B[32m   2572\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2573\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2574\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2575\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2576\u001B[39m )\n\u001B[32m   2577\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2578\u001B[39m     tr_loss_step = \u001B[38;5;28mself\u001B[39m.training_step(model, inputs, num_items_in_batch)\n\u001B[32m   2580\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2581\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2582\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2583\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2584\u001B[39m ):\n\u001B[32m   2585\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2586\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\trainer.py:3792\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3789\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   3791\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m3792\u001B[39m     loss = \u001B[38;5;28mself\u001B[39m.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n\u001B[32m   3794\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   3795\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   3796\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3797\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   3798\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\llm-learn\\Lib\\site-packages\\transformers\\trainer.py:3900\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3898\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3899\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m outputs:\n\u001B[32m-> \u001B[39m\u001B[32m3900\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   3901\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mThe model did not return a loss from the inputs, only the following keys: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3902\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m,\u001B[39m\u001B[33m'\u001B[39m.join(outputs.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. For reference, the inputs it received are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m,\u001B[39m\u001B[33m'\u001B[39m.join(inputs.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3903\u001B[39m         )\n\u001B[32m   3904\u001B[39m     \u001B[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001B[39;00m\n\u001B[32m   3905\u001B[39m     loss = outputs[\u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m outputs[\u001B[32m0\u001B[39m]\n",
      "\u001B[31mValueError\u001B[39m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2dd9c8481160473"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

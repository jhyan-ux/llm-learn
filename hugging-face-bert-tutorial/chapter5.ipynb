{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.251115Z",
     "start_time": "2025-08-03T12:03:30.619772Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./drugsComTest_raw.tsv\",\n",
    "    \"test\": \"./drugsComTrain_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d835eb36120788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.293956Z",
     "start_time": "2025-08-03T12:03:34.270929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [184648, 25268, 172019],\n",
       " 'drugName': ['Efudex', 'Flector Patch', 'Amitiza'],\n",
       " 'condition': ['Basal Cell Carcinoma', 'Pain', 'Irritable Bowel Syndrome'],\n",
       " 'review': ['\"I have BCC on my upper arm and SCC on upper left hand. Unfortunately after 6wks of treatment twice a day the cream didnt work. So disappointed and im now scheduled to have both surgically removed.\"',\n",
       "  '\"I tore my shoulder labrum and the pain can be off the chart.  Hydrocodone and ibuprofen and ice helped some. After my doctor gave me the Flector Patch I noticed major relief in my shoulder within an hour. These work very well. These truly work.\"',\n",
       "  '\"Amitiza is the best if you have ibs!\"'],\n",
       " 'rating': [1.0, 8.0, 10.0],\n",
       " 'date': ['August 30, 2016', 'May 29, 2014', 'July 13, 2016'],\n",
       " 'usefulCount': [16, 40, 9]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a34c581fd57eee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.839197Z",
     "start_time": "2025-08-03T12:03:34.832016Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\"Unnamed: 0\", \"patient_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8de30ccfe472a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.891951Z",
     "start_time": "2025-08-03T12:03:34.850365Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d18fa1b053b342f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:35.104366Z",
     "start_time": "2025-08-03T12:03:35.027481Z"
    }
   },
   "outputs": [],
   "source": [
    "def lowercase_condition(examples):\n",
    "    return {\"condition\": examples[\"condition\"].lower()}\n",
    "\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b785b4d8da6ad3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:51.152551Z",
     "start_time": "2025-08-03T12:03:35.114407Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_review_length(examples):\n",
    "    return {\"review_length\": len(examples[\"review\"].split())}\n",
    "\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6b6872cba11bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:52.897386Z",
     "start_time": "2025-08-03T12:03:51.160895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 46108, 'test': 138514}\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca07607d74e7d4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:00.993958Z",
     "start_time": "2025-08-03T12:03:52.910005Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc261e0738048c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.006278Z",
     "start_time": "2025-08-03T12:04:01.003138Z"
    }
   },
   "outputs": [],
   "source": [
    "# 更改数据格式\n",
    "drug_dataset.set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5cff9ef6a042c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.019573Z",
     "start_time": "2025-08-03T12:04:01.016241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb56fac48efb5495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.049020Z",
     "start_time": "2025-08-03T12:04:01.046522Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_dataset[\"train\"].set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e40dfa3709b849e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:15:43.018406Z",
     "start_time": "2025-08-03T12:15:42.408651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = list(np.unique(drug_dataset[\"train\"][\"condition\"]))\n",
    "labels = [i for i in all_labels if \"span\" not in i]\n",
    "filter_labels = [i for i in all_labels if \"span\" in i]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bd3817e2dde39e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.103379Z",
     "start_time": "2025-08-03T12:04:01.099355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.reset_format()\n",
    "type(drug_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb1199e590e8f9a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:18:19.317926Z",
     "start_time": "2025-08-03T12:18:17.744256Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffle_dataset = drug_dataset[\"train\"].train_test_split(test_size=0.2, train_size=0.8, seed=42)\n",
    "\n",
    "\n",
    "def filter_condition(example):\n",
    "    if example[\"condition\"] in filter_labels:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "shuffle_dataset = shuffle_dataset.filter(filter_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e710780c61cc8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:18:35.631366Z",
     "start_time": "2025-08-03T12:18:35.627861Z"
    }
   },
   "outputs": [],
   "source": [
    "label2Id = {label: i for i, label in enumerate(labels)}\n",
    "id2Label = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42479b14e86836fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:30:11.835534Z",
     "start_time": "2025-08-03T12:30:10.915122Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"review\"], truncation=True)\n",
    "    tokenized_inputs[\"labels\"] = [label2Id[i] for i in examples[\"condition\"]]\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = shuffle_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([o for o in tokenized_datasets.column_names[\"train\"] if o not in [\"labels\", \"input_ids\",\"attention_mask\", \"token_type_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7115ae270ee4069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:52:21.023076Z",
     "start_time": "2025-08-03T12:52:20.895848Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提取样本列表（如取前 4 个样本）\n",
    "samples = tokenized_datasets[\"train\"][: 4]\n",
    "\n",
    "# 生成 Batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6430ec35d960ecd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.940110400Z",
     "start_time": "2025-08-03T11:58:12.685014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用GPU训练: NVIDIA GeForce RTX 4070 SUPER\n",
      "✅ AutoModelForSequenceClassification 导入成功\n",
      "🔄 正在加载BERT模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载到设备: cuda\n",
      "模型参数数量: 109,952,868\n",
      "🧪 测试前向传播...\n",
      "输出logits形状: torch.Size([4, 612])\n",
      "🎉 模型测试成功！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 💪 健壮的模型加载 - 处理导入问题\n",
    "import torch\n",
    "import importlib\n",
    "\n",
    "# 智能设备检测 - 最佳实践！\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ 使用GPU训练: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():  # Apple Silicon Mac\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"✅ 使用Apple Silicon GPU训练\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️  使用CPU训练（会比较慢，但完全可行）\")\n",
    "\n",
    "# 健壮的transformers导入\n",
    "try:\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    print(\"✅ AutoModelForSequenceClassification 导入成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ 导入失败: {e}\")\n",
    "    print(\"🔄 尝试重新加载transformers...\")\n",
    "    \n",
    "    # 强制重新加载\n",
    "    import transformers\n",
    "    importlib.reload(transformers)\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    print(\"✅ 重新加载后导入成功\")\n",
    "\n",
    "# 加载模型\n",
    "print(\"🔄 正在加载BERT模型...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels))\n",
    "model.to(device)\n",
    "\n",
    "print(f\"模型已加载到设备: {device}\")\n",
    "print(f\"模型参数数量: {model.num_parameters():,}\")\n",
    "\n",
    "# 测试模型前向传播\n",
    "print(\"🧪 测试前向传播...\")\n",
    "batch_on_device = {k: v.to(device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = model(**batch_on_device)\n",
    "    print(f\"输出logits形状: {output.logits.shape}\")\n",
    "    print(\"🎉 模型测试成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79313df12c06ab46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.941111600Z",
     "start_time": "2025-08-03T11:43:30.371455Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./train-test\", eval_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e8d2f00e6a3c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.942614200Z",
     "start_time": "2025-08-03T11:45:19.676168Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13767' max='13767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13767/13767 20:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.740200</td>\n",
       "      <td>1.647418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.234500</td>\n",
       "      <td>1.334517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>1.259497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13767, training_loss=1.5380363372915786, metrics={'train_runtime': 1202.0341, 'train_samples_per_second': 91.625, 'train_steps_per_second': 11.453, 'total_flos': 1.057656146851584e+16, 'train_loss': 1.5380363372915786, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38f91a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🚀 从checkpoint加载模型进行预测 ===\n",
      "\n",
      "📁 从 ./train-test/checkpoint-13767 加载训练好的模型...\n",
      "🔧 使用设备: cuda\n",
      "✅ 模型加载成功！\n",
      "📊 模型参数: 109,952,868\n",
      "🏷️  支持类别数: 612\n"
     ]
    }
   ],
   "source": [
    "# 🎯 实际使用训练好的模型进行预测！\n",
    "print(\"=== 🚀 从checkpoint加载模型进行预测 ===\")\n",
    "print()\n",
    "\n",
    "# 使用最终的checkpoint（训练完成的模型）\n",
    "checkpoint_path = \"./train-test/checkpoint-13767\"  # 使用最后一个checkpoint\n",
    "\n",
    "print(f\"📁 从 {checkpoint_path} 加载训练好的模型...\")\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 设备检测\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔧 使用设备: {device}\")\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# 移动到设备并设置评估模式\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"✅ 模型加载成功！\")\n",
    "print(f\"📊 模型参数: {loaded_model.num_parameters():,}\")\n",
    "print(f\"🏷️  支持类别数: {loaded_model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0118b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🔮 创建智能预测函数 ===\n",
      "\n",
      "🏷️  重建标签映射...\n",
      "✅ 预测函数创建完成！支持top-k预测和置信度评估\n"
     ]
    }
   ],
   "source": [
    "# 🔮 创建预测函数\n",
    "print(\"=== 🔮 创建智能预测函数 ===\")\n",
    "print()\n",
    "\n",
    "def predict_drug_condition(review_text, model, tokenizer, id2label_mapping, device, top_k=5):\n",
    "    \"\"\"\n",
    "    预测药物评论对应的疾病类别\n",
    "    \n",
    "    Args:\n",
    "        review_text: 评论文本\n",
    "        model: 训练好的模型\n",
    "        tokenizer: tokenizer\n",
    "        id2label_mapping: ID到标签的映射\n",
    "        device: 计算设备\n",
    "        top_k: 返回前k个最可能的预测\n",
    "    \n",
    "    Returns:\n",
    "        dict: 预测结果\n",
    "    \"\"\"\n",
    "    # 1. 文本预处理\n",
    "    inputs = tokenizer(\n",
    "        review_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 2. 移动到正确设备\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 3. 模型推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # 获得概率分布\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "    # 4. 获取预测结果\n",
    "    probs_cpu = probabilities.cpu().numpy()[0]\n",
    "    \n",
    "    # 5. 找到top-k预测\n",
    "    top_indices = probs_cpu.argsort()[-top_k:][::-1]  # 降序排列\n",
    "    \n",
    "    results = {\n",
    "        'predicted_condition': id2label_mapping[top_indices[0]],\n",
    "        'confidence': float(probs_cpu[top_indices[0]]),\n",
    "        'top_predictions': [\n",
    "            {\n",
    "                'condition': id2label_mapping[idx],\n",
    "                'probability': float(probs_cpu[idx])\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 重建标签映射（从之前的训练中获取）\n",
    "print(\"🏷️  重建标签映射...\")\n",
    "print(f\"✅ 预测函数创建完成！支持top-k预测和置信度评估\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5044f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 🧪 实际预测测试 ===\n",
      "\n",
      "🔍 测试样例准备完成！\n",
      "🚀 开始预测测试...\n",
      "============================================================\n",
      "\n",
      "📝 测试样例 1:\n",
      "评论: This medication really helped with my high blood pressure. After taking it for a month, my readings ...\n",
      "期望类别: 高血压相关\n",
      "🎯 预测结果: high blood pressure\n",
      "📊 置信度: 0.961\n",
      "🏆 前3预测:\n",
      "   1. high blood pressure: 0.961\n",
      "   2. diabetes, type 2: 0.007\n",
      "   3. weight loss: 0.002\n",
      "----------------------------------------\n",
      "\n",
      "📝 测试样例 2:\n",
      "评论: I've been struggling with depression for years. This antidepressant finally gave me relief. I feel m...\n",
      "期望类别: 抑郁症相关\n",
      "🎯 预测结果: depression\n",
      "📊 置信度: 0.951\n",
      "🏆 前3预测:\n",
      "   1. depression: 0.951\n",
      "   2. major depressive disorde: 0.029\n",
      "   3. anxiety and stress: 0.004\n",
      "----------------------------------------\n",
      "\n",
      "📝 测试样例 3:\n",
      "评论: Great pain relief for my chronic back pain. I can finally sleep through the night without waking up ...\n",
      "期望类别: 疼痛相关\n",
      "🎯 预测结果: pain\n",
      "📊 置信度: 0.789\n",
      "🏆 前3预测:\n",
      "   1. pain: 0.789\n",
      "   2. chronic pain: 0.147\n",
      "   3. back pain: 0.032\n",
      "----------------------------------------\n",
      "\n",
      "📝 测试样例 4:\n",
      "评论: This diabetes medication has been a game changer. My blood sugar levels are now stable and I feel mu...\n",
      "期望类别: 糖尿病相关\n",
      "🎯 预测结果: diabetes, type 2\n",
      "📊 置信度: 0.926\n",
      "🏆 前3预测:\n",
      "   1. diabetes, type 2: 0.926\n",
      "   2. diabetes, type 1: 0.021\n",
      "   3. obesity: 0.005\n",
      "----------------------------------------\n",
      "\n",
      "✅ 预测测试完成！\n"
     ]
    }
   ],
   "source": [
    "# 🧪 实际预测测试 - 让我们看看模型学会了什么！\n",
    "print(\"=== 🧪 实际预测测试 ===\")\n",
    "print()\n",
    "\n",
    "# 准备测试用例 - 不同类型的药物评论\n",
    "test_reviews = [\n",
    "    {\n",
    "        \"text\": \"This medication really helped with my high blood pressure. After taking it for a month, my readings went from 150/90 to 120/80. Very satisfied with the results.\",\n",
    "        \"expected\": \"高血压相关\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I've been struggling with depression for years. This antidepressant finally gave me relief. I feel more hopeful and energetic. The side effects were minimal.\",\n",
    "        \"expected\": \"抑郁症相关\"  \n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Great pain relief for my chronic back pain. I can finally sleep through the night without waking up in agony. Highly recommend for pain management.\",\n",
    "        \"expected\": \"疼痛相关\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"This diabetes medication has been a game changer. My blood sugar levels are now stable and I feel much better overall. Good for glucose control.\",\n",
    "        \"expected\": \"糖尿病相关\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🔍 测试样例准备完成！\")\n",
    "print(\"🚀 开始预测测试...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 执行预测\n",
    "for i, test_case in enumerate(test_reviews, 1):\n",
    "    print(f\"\\n📝 测试样例 {i}:\")\n",
    "    print(f\"评论: {test_case['text'][:100]}...\")\n",
    "    print(f\"期望类别: {test_case['expected']}\")\n",
    "    \n",
    "    # 进行预测\n",
    "    try:\n",
    "        result = predict_drug_condition(\n",
    "            test_case['text'], \n",
    "            loaded_model, \n",
    "            loaded_tokenizer, \n",
    "            id2Label,  # 使用之前创建的标签映射\n",
    "            device,\n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        print(f\"🎯 预测结果: {result['predicted_condition']}\")\n",
    "        print(f\"📊 置信度: {result['confidence']:.3f}\")\n",
    "        print(f\"🏆 前3预测:\")\n",
    "        for j, pred in enumerate(result['top_predictions'], 1):\n",
    "            print(f\"   {j}. {pred['condition']}: {pred['probability']:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 预测出错: {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n✅ 预测测试完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4ff7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

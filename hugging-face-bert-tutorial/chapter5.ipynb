{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.251115Z",
     "start_time": "2025-08-03T12:03:30.619772Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./drugsComTest_raw.tsv\",\n",
    "    \"test\": \"./drugsComTrain_raw.tsv\"\n",
    "}\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d835eb36120788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.293956Z",
     "start_time": "2025-08-03T12:03:34.270929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [184648, 25268, 172019],\n",
       " 'drugName': ['Efudex', 'Flector Patch', 'Amitiza'],\n",
       " 'condition': ['Basal Cell Carcinoma', 'Pain', 'Irritable Bowel Syndrome'],\n",
       " 'review': ['\"I have BCC on my upper arm and SCC on upper left hand. Unfortunately after 6wks of treatment twice a day the cream didnt work. So disappointed and im now scheduled to have both surgically removed.\"',\n",
       "  '\"I tore my shoulder labrum and the pain can be off the chart.  Hydrocodone and ibuprofen and ice helped some. After my doctor gave me the Flector Patch I noticed major relief in my shoulder within an hour. These work very well. These truly work.\"',\n",
       "  '\"Amitiza is the best if you have ibs!\"'],\n",
       " 'rating': [1.0, 8.0, 10.0],\n",
       " 'date': ['August 30, 2016', 'May 29, 2014', 'July 13, 2016'],\n",
       " 'usefulCount': [16, 40, 9]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a34c581fd57eee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.839197Z",
     "start_time": "2025-08-03T12:03:34.832016Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\"Unnamed: 0\", \"patient_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8de30ccfe472a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:34.891951Z",
     "start_time": "2025-08-03T12:03:34.850365Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d18fa1b053b342f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:35.104366Z",
     "start_time": "2025-08-03T12:03:35.027481Z"
    }
   },
   "outputs": [],
   "source": [
    "def lowercase_condition(examples):\n",
    "    return {\"condition\": examples[\"condition\"].lower()}\n",
    "\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b785b4d8da6ad3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:51.152551Z",
     "start_time": "2025-08-03T12:03:35.114407Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_review_length(examples):\n",
    "    return {\"review_length\": len(examples[\"review\"].split())}\n",
    "\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6b6872cba11bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:03:52.897386Z",
     "start_time": "2025-08-03T12:03:51.160895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 46108, 'test': 138514}\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca07607d74e7d4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:00.993958Z",
     "start_time": "2025-08-03T12:03:52.910005Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc261e0738048c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.006278Z",
     "start_time": "2025-08-03T12:04:01.003138Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ›´æ”¹æ•°æ®æ ¼å¼\n",
    "drug_dataset.set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5cff9ef6a042c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.019573Z",
     "start_time": "2025-08-03T12:04:01.016241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb56fac48efb5495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.049020Z",
     "start_time": "2025-08-03T12:04:01.046522Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_dataset[\"train\"].set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e40dfa3709b849e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:15:43.018406Z",
     "start_time": "2025-08-03T12:15:42.408651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_labels = list(np.unique(drug_dataset[\"train\"][\"condition\"]))\n",
    "labels = [i for i in all_labels if \"span\" not in i]\n",
    "filter_labels = [i for i in all_labels if \"span\" in i]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bd3817e2dde39e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:01.103379Z",
     "start_time": "2025-08-03T12:04:01.099355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset.reset_format()\n",
    "type(drug_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb1199e590e8f9a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:18:19.317926Z",
     "start_time": "2025-08-03T12:18:17.744256Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffle_dataset = drug_dataset[\"train\"].train_test_split(test_size=0.2, train_size=0.8, seed=42)\n",
    "\n",
    "\n",
    "def filter_condition(example):\n",
    "    if example[\"condition\"] in filter_labels:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "shuffle_dataset = shuffle_dataset.filter(filter_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e710780c61cc8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:18:35.631366Z",
     "start_time": "2025-08-03T12:18:35.627861Z"
    }
   },
   "outputs": [],
   "source": [
    "label2Id = {label: i for i, label in enumerate(labels)}\n",
    "id2Label = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42479b14e86836fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:30:11.835534Z",
     "start_time": "2025-08-03T12:30:10.915122Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"review\"], truncation=True)\n",
    "    tokenized_inputs[\"labels\"] = [label2Id[i] for i in examples[\"condition\"]]\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = shuffle_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([o for o in tokenized_datasets.column_names[\"train\"] if o not in [\"labels\", \"input_ids\",\"attention_mask\", \"token_type_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7115ae270ee4069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:52:21.023076Z",
     "start_time": "2025-08-03T12:52:20.895848Z"
    }
   },
   "outputs": [],
   "source": [
    "# æå–æ ·æœ¬åˆ—è¡¨ï¼ˆå¦‚å–å‰ 4 ä¸ªæ ·æœ¬ï¼‰\n",
    "samples = tokenized_datasets[\"train\"][: 4]\n",
    "\n",
    "# ç”Ÿæˆ Batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6430ec35d960ecd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.940110400Z",
     "start_time": "2025-08-03T11:58:12.685014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä½¿ç”¨GPUè®­ç»ƒ: NVIDIA GeForce RTX 4070 SUPER\n",
      "âœ… AutoModelForSequenceClassification å¯¼å…¥æˆåŠŸ\n",
      "ğŸ”„ æ­£åœ¨åŠ è½½BERTæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: cuda\n",
      "æ¨¡å‹å‚æ•°æ•°é‡: 109,952,868\n",
      "ğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...\n",
      "è¾“å‡ºlogitså½¢çŠ¶: torch.Size([4, 612])\n",
      "ğŸ‰ æ¨¡å‹æµ‹è¯•æˆåŠŸï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’ª å¥å£®çš„æ¨¡å‹åŠ è½½ - å¤„ç†å¯¼å…¥é—®é¢˜\n",
    "import torch\n",
    "import importlib\n",
    "\n",
    "# æ™ºèƒ½è®¾å¤‡æ£€æµ‹ - æœ€ä½³å®è·µï¼\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"âœ… ä½¿ç”¨GPUè®­ç»ƒ: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():  # Apple Silicon Mac\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"âœ… ä½¿ç”¨Apple Silicon GPUè®­ç»ƒ\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸  ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¼šæ¯”è¾ƒæ…¢ï¼Œä½†å®Œå…¨å¯è¡Œï¼‰\")\n",
    "\n",
    "# å¥å£®çš„transformerså¯¼å…¥\n",
    "try:\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    print(\"âœ… AutoModelForSequenceClassification å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”„ å°è¯•é‡æ–°åŠ è½½transformers...\")\n",
    "    \n",
    "    # å¼ºåˆ¶é‡æ–°åŠ è½½\n",
    "    import transformers\n",
    "    importlib.reload(transformers)\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    print(\"âœ… é‡æ–°åŠ è½½åå¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "print(\"ğŸ”„ æ­£åœ¨åŠ è½½BERTæ¨¡å‹...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels))\n",
    "model.to(device)\n",
    "\n",
    "print(f\"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {device}\")\n",
    "print(f\"æ¨¡å‹å‚æ•°æ•°é‡: {model.num_parameters():,}\")\n",
    "\n",
    "# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­\n",
    "print(\"ğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...\")\n",
    "batch_on_device = {k: v.to(device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = model(**batch_on_device)\n",
    "    print(f\"è¾“å‡ºlogitså½¢çŠ¶: {output.logits.shape}\")\n",
    "    print(\"ğŸ‰ æ¨¡å‹æµ‹è¯•æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79313df12c06ab46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.941111600Z",
     "start_time": "2025-08-03T11:43:30.371455Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./train-test\", eval_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e8d2f00e6a3c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:04:14.942614200Z",
     "start_time": "2025-08-03T11:45:19.676168Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13767' max='13767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13767/13767 20:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.740200</td>\n",
       "      <td>1.647418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.234500</td>\n",
       "      <td>1.334517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>1.259497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "d:\\software\\miniconda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13767, training_loss=1.5380363372915786, metrics={'train_runtime': 1202.0341, 'train_samples_per_second': 91.625, 'train_steps_per_second': 11.453, 'total_flos': 1.057656146851584e+16, 'train_loss': 1.5380363372915786, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38f91a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸš€ ä»checkpointåŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹ ===\n",
      "\n",
      "ğŸ“ ä» ./train-test/checkpoint-13767 åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...\n",
      "ğŸ”§ ä½¿ç”¨è®¾å¤‡: cuda\n",
      "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "ğŸ“Š æ¨¡å‹å‚æ•°: 109,952,868\n",
      "ğŸ·ï¸  æ”¯æŒç±»åˆ«æ•°: 612\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ å®é™…ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼\n",
    "print(\"=== ğŸš€ ä»checkpointåŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹ ===\")\n",
    "print()\n",
    "\n",
    "# ä½¿ç”¨æœ€ç»ˆçš„checkpointï¼ˆè®­ç»ƒå®Œæˆçš„æ¨¡å‹ï¼‰\n",
    "checkpoint_path = \"./train-test/checkpoint-13767\"  # ä½¿ç”¨æœ€åä¸€ä¸ªcheckpoint\n",
    "\n",
    "print(f\"ğŸ“ ä» {checkpoint_path} åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# è®¾å¤‡æ£€æµ‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®è¯„ä¼°æ¨¡å¼\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "print(f\"ğŸ“Š æ¨¡å‹å‚æ•°: {loaded_model.num_parameters():,}\")\n",
    "print(f\"ğŸ·ï¸  æ”¯æŒç±»åˆ«æ•°: {loaded_model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0118b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ”® åˆ›å»ºæ™ºèƒ½é¢„æµ‹å‡½æ•° ===\n",
      "\n",
      "ğŸ·ï¸  é‡å»ºæ ‡ç­¾æ˜ å°„...\n",
      "âœ… é¢„æµ‹å‡½æ•°åˆ›å»ºå®Œæˆï¼æ”¯æŒtop-ké¢„æµ‹å’Œç½®ä¿¡åº¦è¯„ä¼°\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”® åˆ›å»ºé¢„æµ‹å‡½æ•°\n",
    "print(\"=== ğŸ”® åˆ›å»ºæ™ºèƒ½é¢„æµ‹å‡½æ•° ===\")\n",
    "print()\n",
    "\n",
    "def predict_drug_condition(review_text, model, tokenizer, id2label_mapping, device, top_k=5):\n",
    "    \"\"\"\n",
    "    é¢„æµ‹è¯ç‰©è¯„è®ºå¯¹åº”çš„ç–¾ç—…ç±»åˆ«\n",
    "    \n",
    "    Args:\n",
    "        review_text: è¯„è®ºæ–‡æœ¬\n",
    "        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "        tokenizer: tokenizer\n",
    "        id2label_mapping: IDåˆ°æ ‡ç­¾çš„æ˜ å°„\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        top_k: è¿”å›å‰kä¸ªæœ€å¯èƒ½çš„é¢„æµ‹\n",
    "    \n",
    "    Returns:\n",
    "        dict: é¢„æµ‹ç»“æœ\n",
    "    \"\"\"\n",
    "    # 1. æ–‡æœ¬é¢„å¤„ç†\n",
    "    inputs = tokenizer(\n",
    "        review_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 2. ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 3. æ¨¡å‹æ¨ç†\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # è·å¾—æ¦‚ç‡åˆ†å¸ƒ\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "    # 4. è·å–é¢„æµ‹ç»“æœ\n",
    "    probs_cpu = probabilities.cpu().numpy()[0]\n",
    "    \n",
    "    # 5. æ‰¾åˆ°top-ké¢„æµ‹\n",
    "    top_indices = probs_cpu.argsort()[-top_k:][::-1]  # é™åºæ’åˆ—\n",
    "    \n",
    "    results = {\n",
    "        'predicted_condition': id2label_mapping[top_indices[0]],\n",
    "        'confidence': float(probs_cpu[top_indices[0]]),\n",
    "        'top_predictions': [\n",
    "            {\n",
    "                'condition': id2label_mapping[idx],\n",
    "                'probability': float(probs_cpu[idx])\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# é‡å»ºæ ‡ç­¾æ˜ å°„ï¼ˆä»ä¹‹å‰çš„è®­ç»ƒä¸­è·å–ï¼‰\n",
    "print(\"ğŸ·ï¸  é‡å»ºæ ‡ç­¾æ˜ å°„...\")\n",
    "print(f\"âœ… é¢„æµ‹å‡½æ•°åˆ›å»ºå®Œæˆï¼æ”¯æŒtop-ké¢„æµ‹å’Œç½®ä¿¡åº¦è¯„ä¼°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5044f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ§ª å®é™…é¢„æµ‹æµ‹è¯• ===\n",
      "\n",
      "ğŸ” æµ‹è¯•æ ·ä¾‹å‡†å¤‡å®Œæˆï¼\n",
      "ğŸš€ å¼€å§‹é¢„æµ‹æµ‹è¯•...\n",
      "============================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•æ ·ä¾‹ 1:\n",
      "è¯„è®º: This medication really helped with my high blood pressure. After taking it for a month, my readings ...\n",
      "æœŸæœ›ç±»åˆ«: é«˜è¡€å‹ç›¸å…³\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ: high blood pressure\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 0.961\n",
      "ğŸ† å‰3é¢„æµ‹:\n",
      "   1. high blood pressure: 0.961\n",
      "   2. diabetes, type 2: 0.007\n",
      "   3. weight loss: 0.002\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ æµ‹è¯•æ ·ä¾‹ 2:\n",
      "è¯„è®º: I've been struggling with depression for years. This antidepressant finally gave me relief. I feel m...\n",
      "æœŸæœ›ç±»åˆ«: æŠ‘éƒç—‡ç›¸å…³\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ: depression\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 0.951\n",
      "ğŸ† å‰3é¢„æµ‹:\n",
      "   1. depression: 0.951\n",
      "   2. major depressive disorde: 0.029\n",
      "   3. anxiety and stress: 0.004\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ æµ‹è¯•æ ·ä¾‹ 3:\n",
      "è¯„è®º: Great pain relief for my chronic back pain. I can finally sleep through the night without waking up ...\n",
      "æœŸæœ›ç±»åˆ«: ç–¼ç—›ç›¸å…³\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ: pain\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 0.789\n",
      "ğŸ† å‰3é¢„æµ‹:\n",
      "   1. pain: 0.789\n",
      "   2. chronic pain: 0.147\n",
      "   3. back pain: 0.032\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ æµ‹è¯•æ ·ä¾‹ 4:\n",
      "è¯„è®º: This diabetes medication has been a game changer. My blood sugar levels are now stable and I feel mu...\n",
      "æœŸæœ›ç±»åˆ«: ç³–å°¿ç—…ç›¸å…³\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ: diabetes, type 2\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 0.926\n",
      "ğŸ† å‰3é¢„æµ‹:\n",
      "   1. diabetes, type 2: 0.926\n",
      "   2. diabetes, type 1: 0.021\n",
      "   3. obesity: 0.005\n",
      "----------------------------------------\n",
      "\n",
      "âœ… é¢„æµ‹æµ‹è¯•å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª å®é™…é¢„æµ‹æµ‹è¯• - è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹å­¦ä¼šäº†ä»€ä¹ˆï¼\n",
    "print(\"=== ğŸ§ª å®é™…é¢„æµ‹æµ‹è¯• ===\")\n",
    "print()\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•ç”¨ä¾‹ - ä¸åŒç±»å‹çš„è¯ç‰©è¯„è®º\n",
    "test_reviews = [\n",
    "    {\n",
    "        \"text\": \"This medication really helped with my high blood pressure. After taking it for a month, my readings went from 150/90 to 120/80. Very satisfied with the results.\",\n",
    "        \"expected\": \"é«˜è¡€å‹ç›¸å…³\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I've been struggling with depression for years. This antidepressant finally gave me relief. I feel more hopeful and energetic. The side effects were minimal.\",\n",
    "        \"expected\": \"æŠ‘éƒç—‡ç›¸å…³\"  \n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Great pain relief for my chronic back pain. I can finally sleep through the night without waking up in agony. Highly recommend for pain management.\",\n",
    "        \"expected\": \"ç–¼ç—›ç›¸å…³\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"This diabetes medication has been a game changer. My blood sugar levels are now stable and I feel much better overall. Good for glucose control.\",\n",
    "        \"expected\": \"ç³–å°¿ç—…ç›¸å…³\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ” æµ‹è¯•æ ·ä¾‹å‡†å¤‡å®Œæˆï¼\")\n",
    "print(\"ğŸš€ å¼€å§‹é¢„æµ‹æµ‹è¯•...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ‰§è¡Œé¢„æµ‹\n",
    "for i, test_case in enumerate(test_reviews, 1):\n",
    "    print(f\"\\nğŸ“ æµ‹è¯•æ ·ä¾‹ {i}:\")\n",
    "    print(f\"è¯„è®º: {test_case['text'][:100]}...\")\n",
    "    print(f\"æœŸæœ›ç±»åˆ«: {test_case['expected']}\")\n",
    "    \n",
    "    # è¿›è¡Œé¢„æµ‹\n",
    "    try:\n",
    "        result = predict_drug_condition(\n",
    "            test_case['text'], \n",
    "            loaded_model, \n",
    "            loaded_tokenizer, \n",
    "            id2Label,  # ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„æ ‡ç­¾æ˜ å°„\n",
    "            device,\n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¯ é¢„æµ‹ç»“æœ: {result['predicted_condition']}\")\n",
    "        print(f\"ğŸ“Š ç½®ä¿¡åº¦: {result['confidence']:.3f}\")\n",
    "        print(f\"ğŸ† å‰3é¢„æµ‹:\")\n",
    "        for j, pred in enumerate(result['top_predictions'], 1):\n",
    "            print(f\"   {j}. {pred['condition']}: {pred['probability']:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é¢„æµ‹å‡ºé”™: {e}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nâœ… é¢„æµ‹æµ‹è¯•å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4ff7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
